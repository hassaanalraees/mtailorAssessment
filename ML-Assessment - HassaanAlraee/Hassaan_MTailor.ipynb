{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3Gqri28DCa4"
      },
      "outputs": [],
      "source": [
        "! pip install bs4 lxml kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'bilalyousaf0014'\n",
        "os.environ['KAGGLE_KEY'] = '11031bc21c5e3ec23585dbe17dc4267d'"
      ],
      "metadata": {
        "id": "EDqEphJXDNop"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d bilalyousaf0014/ml-engineer-assessment-dataset"
      ],
      "metadata": {
        "id": "8iHFGHMLDQ3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/ml-engineer-assessment-dataset.zip"
      ],
      "metadata": {
        "id": "GHD3mj4GDRe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.models import resnet18, ResNet18_Weights"
      ],
      "metadata": {
        "id": "KzSeUp8sDY3B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Model:"
      ],
      "metadata": {
        "id": "hWR5rKdEDl1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        #pretrained_model = resnet18(pretrained=True)\n",
        "        pretrained_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "        self.backbone = nn.Sequential(*list(pretrained_model.children())[:-2])\n",
        "\n",
        "        backbone_output_size = 512 * 7 * 7\n",
        "\n",
        "        # Initialize the required Layers\n",
        "        self.have_object = nn.Linear(backbone_output_size, 1)\n",
        "        self.bbox = nn.Linear(backbone_output_size, 4)\n",
        "        self.cat_or_dog = nn.Linear(backbone_output_size, 1)\n",
        "        self.specie = nn.Linear(backbone_output_size, 9)\n",
        "\n",
        "#intialization for the activation functions\n",
        "    def forward(self, input):\n",
        "        out_backbone = self.backbone(input)\n",
        "        out_backbone = torch.flatten(out_backbone, start_dim=1)\n",
        "\n",
        "        \n",
        "        have_object = torch.sigmoid(self.have_object(out_backbone))\n",
        "        bbox = self.bbox(out_backbone) #does not need any \n",
        "\n",
        "      \n",
        "        cat_or_dog = torch.sigmoid(self.cat_or_dog(out_backbone))\n",
        "        specie = torch.softmax(self.specie(out_backbone), dim=1) #multiclass \n",
        "\n",
        "        return {\n",
        "            \"bbox\": bbox,\n",
        "            \"object\": have_object,\n",
        "            \"cat_or_dog\": cat_or_dog,\n",
        "            \"specie\": specie\n",
        "        }"
      ],
      "metadata": {
        "id": "mR7yXzKXDeMh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CUSTOM DATALOADER IMPLEMENTATION"
      ],
      "metadata": {
        "id": "CssYBNF_DqR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = np.load('/content/assessment_dataset/train_list.npy', allow_pickle=True).tolist()\n",
        "val_list = np.load('/content/assessment_dataset/val_list.npy', allow_pickle=True).tolist()"
      ],
      "metadata": {
        "id": "HiGEH6aGDjqJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have tweaked the xml reader a little bit as we needed have_object and dynamic width & heights"
      ],
      "metadata": {
        "id": "HwCEvpIwwbXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_xml_file(path):\n",
        "    with open(path, 'r') as f:\n",
        "        data = f.read()\n",
        "    bs_data = BeautifulSoup(data, 'xml')\n",
        "    width = int(bs_data.find(\"width\").text)\n",
        "    height = int(bs_data.find(\"height\").text)\n",
        "    return {\n",
        "        \"have_object\": bs_data.find(\"name\") is not None,\n",
        "        \"cat_or_dog\": bs_data.find(\"name\").text,\n",
        "        \"xmin\": int(bs_data.find(\"xmin\").text),\n",
        "        \"ymin\": int(bs_data.find(\"ymin\").text),\n",
        "        \"xmax\": int(bs_data.find(\"xmax\").text),\n",
        "        \"ymax\": int(bs_data.find(\"ymax\").text),\n",
        "        \"specie\": \"_\".join(path.split(os.sep)[-1].split(\"_\")[:-1]),\n",
        "        \"width\": width,\n",
        "        \"height\": height\n",
        "    }"
      ],
      "metadata": {
        "id": "E5qrl-bUDwnW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below data loader has many tweaks\n",
        "\n",
        "1.   Needed a check for missing xmls otherwise it was giving errors\n",
        "2.   Label dictionary values\n",
        "3.   Pre-Processing for the raw image\n",
        "\n"
      ],
      "metadata": {
        "id": "bZa6BZehwsOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class CustomDataset():\n",
        "\n",
        "  def __init__(self, dataset_path, images_list, train=False):\n",
        "    self.data_source = dataset_path\n",
        "    self.images_list = images_list\n",
        "    self.train = train\n",
        "\n",
        "    image_folder_path = os.path.join(dataset_path, \"images\")\n",
        "    label_folder_path = os.path.join(dataset_path, \"labels\")\n",
        "\n",
        "    # Define preprocessing transform\n",
        "    self.preprocess = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    self.samples = []\n",
        "    for path in os.listdir(image_folder_path):\n",
        "        name = path.split(os.sep)[-1].split(\".\")[0]\n",
        "        if name in images_list:\n",
        "          try:\n",
        "            xml_path = os.path.join(label_folder_path, name+\".xml\")\n",
        "            xml_data = read_xml_file(xml_path)\n",
        "            \n",
        "          except FileNotFoundError:\n",
        "            #print(f\"No XML file found for {name}. Skipping...\")\n",
        "            continue\n",
        "          try:\n",
        "            image_path = os.path.join(image_folder_path, name+\".jpg\")\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            \n",
        "          except FileNotFoundError:\n",
        "            #print(f\"No JPG file found for {name}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "          if self.preprocess is not None:\n",
        "            image = self.preprocess(image)\n",
        "          \n",
        "\n",
        "          labels = {\n",
        "              \"have_object\": xml_data[\"have_object\"],\n",
        "              \"cat_or_dog\": xml_data[\"cat_or_dog\"],\n",
        "              \"have_object\": xml_data[\"have_object\"],\n",
        "              \"specie\": xml_data[\"specie\"],\n",
        "              \"bbox\": [xml_data[\"xmin\"], xml_data[\"ymin\"], xml_data[\"xmax\"], xml_data[\"ymax\"]],\n",
        "              \"width\": xml_data[\"width\"],\n",
        "              \"height\": xml_data[\"height\"],\n",
        "              \"xmin\": xml_data[\"xmin\"],\n",
        "              \"ymin\": xml_data[\"ymin\"],\n",
        "              \"xmax\": xml_data[\"xmax\"],\n",
        "              \"ymax\": xml_data[\"ymax\"]\n",
        "\n",
        "          }\n",
        "          self.samples.append((image, labels))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image, label = self.samples[index]\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "TK5E3JzJD0ON"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below test function has also multiple additions:\n",
        "\n",
        "1.   We needed built-in functions for converting the tensors to list, numpy and a NumPy array and then into a Python list for handling or serialization\n",
        "2.   Then we needed pre-processing for each prediction output. For binary ones, we need to compare the thresholds of 0.5 and replace values greater than 0.5 to 1.\n",
        "3. For multi-class, we needed to apply torch.argmax() along with second dimension (dim=1) to obtain the index of the class with the highest probability.\n",
        "\n"
      ],
      "metadata": {
        "id": "0MicYHToxRKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def test(model):\n",
        "\n",
        "  def post_process_object(x):\n",
        "    return torch.where(x > 0.5, 1.0, 0.0).squeeze(1) #replaces values greater than 0.5 with 1.0\n",
        "\n",
        "  def post_process_cat_or_dog(x):\n",
        "    return torch.where(x > 0.5, 1.0, 0.0).squeeze(1)\n",
        "\n",
        "  def post_process_specie(x):\n",
        "    return torch.argmax(x, dim=1)\n",
        "\n",
        "  def post_process_bbox(x):\n",
        "    return torch.argmax(x, dim=1) # index of the class with the highest probability\n",
        "\n",
        "  def __tl(x):\n",
        "    return x.tolist()\n",
        "\n",
        "  def __tn(x):\n",
        "    return x.detach().cpu().numpy()\n",
        "\n",
        "  def __tnl(x):\n",
        "    return (x.detach().cpu().numpy()).tolist()\n",
        "\n",
        "\n",
        "  val_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=val_list)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "  metric_object = torchmetrics.F1Score(task=\"binary\") #F1 because it handles both precision & recall\n",
        "  metric_cat_or_dog = torchmetrics.F1Score(task=\"binary\")\n",
        "  metric_specie = torchmetrics.F1Score(task=\"multiclass\", num_classes=9) #F1 works best with multiclass as well\n",
        "  metric_bbox = torchmetrics.F1Score(task=\"multiclass\", num_classes=4)\n",
        "\n",
        "  output_list = {\n",
        "      \"object\": [],\n",
        "      \"cat_or_dog\": [],\n",
        "      \"specie\": [],\n",
        "      \"bbox\": []\n",
        "  }\n",
        "  labels_list = {\n",
        "      \"object\": [],\n",
        "      \"cat_or_dog\": [],\n",
        "      \"specie\": [],\n",
        "      \"bbox\": []\n",
        "  }\n",
        "  for i, data in enumerate(val_loader):\n",
        "    inputs, labels = data\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      inputs = inputs.cuda()\n",
        "      labels = {key: value.cuda() for key, value in labels.items()}\n",
        "\n",
        "    # Make predictions for this batch\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    is_object = __tnl(labels[\"have_object\"])\n",
        "    width = __tn(labels[\"width\"])\n",
        "    height = __tn(labels[\"height\"])\n",
        "    output_list[\"object\"].extend(__tnl(post_process_object(outputs[\"object\"])))\n",
        "    labels_list[\"object\"].extend(__tnl(labels[\"have_object\"]))\n",
        "\n",
        "    if is_object[0] == 1.0:\n",
        "\n",
        "      label_encoder = LabelEncoder()\n",
        "      encode_cat_or_dog = label_encoder.fit_transform(labels[\"cat_or_dog\"])\n",
        "      cat_or_dog_Target = torch.tensor(encode_cat_or_dog)\n",
        "\n",
        "      output_list[\"cat_or_dog\"].extend(\n",
        "        __tnl(post_process_cat_or_dog(outputs[\"cat_or_dog\"]))\n",
        "      )\n",
        "      labels_list[\"cat_or_dog\"].extend(\n",
        "        __tnl(cat_or_dog_Target)\n",
        "      )\n",
        "      \n",
        "      label_encoder = LabelEncoder()\n",
        "      encode_cat_or_dog = label_encoder.fit_transform(labels_list[\"cat_or_dog\"])\n",
        "      cat_or_dog_Target = torch.tensor(encode_cat_or_dog)\n",
        "\n",
        "      label_encoder = LabelEncoder()\n",
        "      encode_specie = label_encoder.fit_transform(labels[\"specie\"])\n",
        "      specie_Target = torch.tensor(encode_specie)\n",
        "\n",
        "      output_list[\"specie\"].extend(\n",
        "        __tnl(post_process_specie(outputs[\"specie\"]))\n",
        "      )\n",
        "      labels_list[\"specie\"].extend(__tnl(specie_Target))\n",
        "      \n",
        "      label_encoder = LabelEncoder()\n",
        "      encode_specie = label_encoder.fit_transform(labels_list[\"specie\"])\n",
        "      specie_Target = torch.tensor(encode_specie)\n",
        "\n",
        "      '''labels_bbox = torch.stack(labels[\"bbox\"]).float().transpose(0, 1)\n",
        "      \n",
        "\n",
        "      output_list[\"bbox\"].extend(\n",
        "        __tnl(post_process_bbox(outputs[\"bbox\"]))\n",
        "      )\n",
        "      labels_list[\"bbox\"].extend(__tnl(labels_bbox))\n",
        "\n",
        "      labels_bbox = tuple(torch.tensor(bbox) for bbox in labels_list[\"bbox\"])\n",
        "      bbox_Target = torch.stack(labels_bbox).float().transpose(0, 1)'''\n",
        "\n",
        "\n",
        "  score_object = metric_object(torch.tensor(output_list[\"object\"]), torch.tensor(labels_list[\"object\"]))\n",
        "  score_cat_or_dog = metric_cat_or_dog(torch.tensor(output_list[\"cat_or_dog\"]), cat_or_dog_Target)\n",
        "  score_specie = metric_specie(torch.tensor(output_list[\"specie\"]), specie_Target)\n",
        "  #score_bbox = metric_bbox(torch.tensor(output_list[\"bbox\"]), torch.tensor(bbox_Target))\n",
        "\n",
        "  return score_object, score_cat_or_dog, score_specie #returning the score for each output"
      ],
      "metadata": {
        "id": "5VklT1j-D7vC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "tdSRIbSLEAhk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def train(epochs, model_weights):\n",
        "\n",
        "  # Initialize Model and Optimizer\n",
        "  model = Model()\n",
        "  optimizer = Adam(model.parameters())\n",
        "\n",
        "  # Initialize Loss Functions\n",
        "  have_object_loss = nn.BCELoss() #because it is a binary object\n",
        "  specie_loss = nn.CrossEntropyLoss() # suitable for multi-class classification\n",
        "  cat_or_dog_loss = nn.CrossEntropyLoss()\n",
        "  bbox_loss = nn.MSELoss() # MSE perfect for bbox to measure difference between the predicted and target values\n",
        "  xmin_loss = nn.MSELoss()\n",
        "  ymin_loss = nn.MSELoss()\n",
        "  xmax_loss = nn.MSELoss()\n",
        "  ymax_loss = nn.MSELoss()\n",
        "\n",
        "  #resume training initialize the best loss with a high value to track best loss achieved so far\n",
        "  # after each epoch, the code will check if the current epoch loss is lower than best_loss\n",
        "  #also, the code saves the model weights only when a new best loss is achieved during training.\n",
        "  if model_weights is not None:\n",
        "        model.load_state_dict(torch.load(model_weights))\n",
        "        best_loss = float('inf') \n",
        "  else:\n",
        "    best_loss = None \n",
        "\n",
        "  training_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=train_list)\n",
        "  training_loader = training_loader = DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "  def train_one_epoch(epoch_index, tb_writer):\n",
        "      running_loss = 0.\n",
        "      last_loss = 0.\n",
        "\n",
        "      # Here, we use enumerate(training_loader) instead of\n",
        "      # iter(training_loader) so that we can track the batch\n",
        "      # index and do some intra-epoch reporting\n",
        "      for i, data in enumerate(training_loader):\n",
        "          # Every data instance is an input + label pair\n",
        "          inputs, labels = data\n",
        "\n",
        "          if torch.cuda.is_available():\n",
        "                inputs = inputs.cuda()\n",
        "                labels = {key: value.cuda() for key, value in labels.items()}\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Make predictions for this batch\n",
        "          outputs = model(inputs)\n",
        "         \n",
        "\n",
        "          # Compute the loss and its gradients\n",
        "          #Have object\n",
        "          target_haveobject = labels[\"have_object\"].unsqueeze(1).float()\n",
        "          loss_have_object = have_object_loss(outputs[\"object\"].float(), target_haveobject.float())\n",
        "          \n",
        "          #specie loss\n",
        "          label_encoder = LabelEncoder() #strings cannot be directly converted to tensors so we need to encode\n",
        "          encoded_specie = label_encoder.fit_transform(labels[\"specie\"])\n",
        "          specie_target = torch.tensor(encoded_specie)\n",
        "          \n",
        "          loss_specie = specie_loss(outputs[\"specie\"], specie_target)\n",
        "          \n",
        "          #cat or dog loss \n",
        "          label_encoder = LabelEncoder()\n",
        "          encode_cat_or_dog = label_encoder.fit_transform(labels[\"cat_or_dog\"])\n",
        "          cat_or_dog_Target = torch.tensor(encode_cat_or_dog)\n",
        "\n",
        "          cat_or_dog_Target = cat_or_dog_Target.unsqueeze(1).float()\n",
        "\n",
        "          labels_bbox = torch.stack(labels[\"bbox\"]).float().transpose(0, 1) #converting the labels so that it is same as outpit\n",
        "          loss_bbox = bbox_loss(outputs[\"bbox\"], labels_bbox) #loss bbox \n",
        "\n",
        "          loss_cat_or_dog = cat_or_dog_loss(outputs[\"cat_or_dog\"], cat_or_dog_Target.float())\n",
        "          \n",
        "          #loss of coordinates\n",
        "          #we need them all in float also we need the dimensions same as output for labels \n",
        "          X_min_target = torch.stack(labels[\"bbox\"])[:, 0].repeat(inputs.size(0)).float()[:outputs[\"bbox\"].shape[0]]\n",
        "          #print(\"X_min_target shape:\", X_min_target)\n",
        "          #print(\"outputs[\\\"bbox\\\"][:, 0] shape:\", outputs[\"bbox\"][:, 0])\n",
        "          loss_xmin = xmin_loss(outputs[\"bbox\"][:, 0], X_min_target)\n",
        "\n",
        "\n",
        "          y_min_target = torch.stack(labels[\"bbox\"])[:, 1].repeat(inputs.size(0)).float()[:outputs[\"bbox\"].shape[0]]\n",
        "          #print(\"y_min_target shape:\", y_min_target)\n",
        "          #print(\"outputs[\\\"bbox\\\"][:, 1] shape:\", outputs[\"bbox\"][:, 1])\n",
        "          loss_ymin = ymin_loss(outputs[\"bbox\"][:, 1], y_min_target)\n",
        "\n",
        "          x_max_target = torch.stack(labels[\"bbox\"])[:, 2].repeat(inputs.size(0)).float()[:outputs[\"bbox\"].shape[0]]\n",
        "          #print(\"x_max_target shape:\", x_max_target)\n",
        "          #print(\"outputs[\\\"bbox\\\"][:, 2] shape:\", outputs[\"bbox\"][:, 2])\n",
        "          loss_xmax = xmax_loss(outputs[\"bbox\"][:, 2], x_max_target)\n",
        "\n",
        "          loss_ymax_target = torch.stack(labels[\"bbox\"])[:, 3].repeat(inputs.size(0)).float()[:outputs[\"bbox\"].shape[0]]\n",
        "          #print(\"loss_ymax_target shape:\", loss_ymax_target)\n",
        "          #print(\"outputs[\\\"bbox\\\"][:, 3] shape:\", outputs[\"bbox\"][:, 3])\n",
        "          loss_ymax = ymax_loss(outputs[\"bbox\"][:, 3], loss_ymax_target)\n",
        "\n",
        "          loss = (\n",
        "                loss_have_object\n",
        "                + loss_specie\n",
        "                + loss_cat_or_dog\n",
        "                + loss_bbox\n",
        "                + loss_xmin\n",
        "                + loss_ymin\n",
        "                + loss_xmax\n",
        "                + loss_ymax\n",
        "            )\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Gather data and report\n",
        "          running_loss += loss.item()\n",
        "          if i % 10 == 0:\n",
        "              last_loss = running_loss / 10 # loss per batch\n",
        "              running_loss = 0.\n",
        "      return last_loss\n",
        "\n",
        "  for i in range(epochs):\n",
        "    \n",
        "\n",
        "    epoch_loss = train_one_epoch(i, None)\n",
        "    print(f' Epoch {i} Loss : {epoch_loss}')\n",
        "    #path = \"/content/Model.pth\"\n",
        "\n",
        "    if best_loss is None or epoch_loss < best_loss: # checks if the current epoch loss is lower than best_loss\n",
        "      best_loss = epoch_loss\n",
        "      folder_path = '/content'\n",
        "      file_name = 'Model'+str(i)+'.pth'\n",
        "\n",
        "      # Combine the folder path and file name to create the complete file path\n",
        "      file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "      torch.save(model.state_dict(), file_path)\n",
        "\n",
        "    metrics = test(model)\n",
        "    print(metrics)"
      ],
      "metadata": {
        "id": "89OKyo4wENVh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(50,model_weights=None)"
      ],
      "metadata": {
        "id": "rGgSQtM-GAhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the visualize function and it does the following:\n",
        "\n",
        "*   There is an input path, output path, and weight path\n",
        "*   The output function returns { \"has_object\": True, \"cat_or_dog\": \"cat\", \"specie\": \"persian\", \"xmin\": 10, \"ymin\": 10, \"xmax\": 10, \"ymax\": 10 }\n",
        "* images have extension jpg or jpeg\n",
        "* the output folder have images stored with bounding box drawn on them\n",
        "\n"
      ],
      "metadata": {
        "id": "_B6ScA4J0xTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from pathlib import Path\n",
        "\n",
        "def visualize_images(input_folder, weight_file, output_folder):\n",
        "   \n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    \n",
        "    model = Model()  \n",
        "    model.load_state_dict(torch.load(weight_file))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Define the image transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    \n",
        "    for filename in os.listdir(input_folder):\n",
        "       \n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        file_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "        image = Image.open(os.path.join(input_folder, filename))\n",
        "\n",
        "        try:\n",
        "          image = Image.open(os.path.join(input_folder, file_name+\".jpg\"))\n",
        "        except:\n",
        "          image = Image.open(os.path.join(input_folder, file_name+\".jpeg\"))\n",
        "        if image is not None:\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                output = model(image_tensor)\n",
        "\n",
        "          \n",
        "            bboxes = output['bbox']  \n",
        "            specie = output['specie']\n",
        "            cat_or_dog = output['cat_or_dog']\n",
        "            have_object = output['object']\n",
        "\n",
        "            \n",
        "\n",
        "            \n",
        "            #fig, ax = plt.subplots()\n",
        "            #ax.imshow(image)\n",
        "            draw = ImageDraw.Draw(image)\n",
        "            for bbox in bboxes:\n",
        "                xmin, ymin, xmax, ymax = bbox\n",
        "                border_width = 100  #optional\n",
        "                xmin = xmin - border_width\n",
        "                ymin = ymin - border_width\n",
        "                xmax = xmax + border_width\n",
        "                ymax = ymax + border_width\n",
        "                #rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none')\n",
        "                draw.rectangle([xmin, ymin, xmax, ymax], outline='red', width=2)\n",
        "                draw.text((bbox[0], bbox[1] - 20),text=file_name, fill='red')\n",
        "                #ax.add_patch(rect)\n",
        "            #plt.axis('off')\n",
        "            plt.imshow(image)\n",
        "            plt.show()\n",
        "            #plt.savefig(os.path.join(output_folder, filename))  # Save the image with bounding boxes\n",
        "            #plt.show()  # Show the image with bounding boxes\n",
        "        else:\n",
        "          continue\n",
        "            \n",
        "\n",
        "    print(\"Visualization complete.\")"
      ],
      "metadata": {
        "id": "cAfP_3TkcLFD"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Output = \"/content/Output\" #output folder\n",
        "Images = \"/content/assessment_dataset/images\" #images folder\n",
        "weight_file = \"/content/Model14.pth\" #weight file\n",
        "\n",
        "visualize_images(Images,weight_file,Output)"
      ],
      "metadata": {
        "id": "OGCjjwhykLTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}